{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2922060f",
   "metadata": {},
   "source": [
    "## Environment Setup Verification\n",
    "This cell checks the versions of scikit-learn and XGBoost to ensure compatibility. Different versions of these machine learning libraries can sometimes behave differently, so it's important to verify we're using the expected versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a927bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n",
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import sklearn # Check Sklearn version\n",
    "import xgboost\n",
    "print(sklearn.__version__)\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee5367",
   "metadata": {},
   "source": [
    "## Directory Inspection\n",
    "This command lists files in the current directory. We use it to verify that required files (like model.pkl) are present in our working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0a04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker-pulled-image-as-base  main.ipynb  model.pkl     __pycache__\n",
      "documented_main.ipynb\t     main.py\t model.tar.gz  requirements.txt\n",
      "env\t\t\t     model\t predict.py    transformers.py\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd484e4",
   "metadata": {},
   "source": [
    "## Model File Preparation\n",
    "Copies our pre-trained model file to a specific directory structure required by Amazon SageMaker. This is part of preparing our model for deployment in the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf66ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp model.pkl /opt/ml/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe36d33",
   "metadata": {},
   "source": [
    "## SageMaker Initialization\n",
    "This cell sets up the fundamental components for working with SageMaker:\n",
    "- Connects to AWS services using boto3\n",
    "- Creates a SageMaker session\n",
    "- Specifies our S3 bucket for model storage\n",
    "- Imports necessary machine learning libraries\n",
    "The print statements help verify our environment is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d631bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/02/25 14:54:36] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/02/25 14:54:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=597204;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=624236;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/xdg-ubuntu/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/murivirg/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/02/25 14:54:37] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/02/25 14:54:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=615932;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=397652;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucket mainbucketrockhight5461\n",
      "sagemaker version: 2.242.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "import datetime\n",
    "import time\n",
    "import tarfile\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = 'mainbucketrockhight5461' # Mention the created S3 bucket name here\n",
    "print(\"Using bucket \" + bucket)\n",
    "# hi\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e15044",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "Here we load our pre-trained machine learning model from a pickle file. Pickle is a Python format for saving objects, in this case our trained model pipeline. This is the first step to start making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560390c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a806d54",
   "metadata": {},
   "source": [
    "## Model Inspection\n",
    "These print statements help us verify:\n",
    "1. What type of model we've loaded (e.g., XGBoost classifier, Scikit-learn pipeline)\n",
    "2. The model's configuration parameters\n",
    "This is crucial for debugging and ensuring we have the right model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ca96c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Pipeline(steps=[('processing',\n",
      "                 <transformers.RawDataProcessor object at 0x715689754050>),\n",
      "                ('slice_columns',\n",
      "                 <transformers.DataSlicer object at 0x7156897570e0>),\n",
      "                ('null_filling',\n",
      "                 <transformers.NullFillTransformer object at 0x715689757e00>),\n",
      "                ('model',\n",
      "                 FitModel(folds=5,\n",
      "                          hyper_parameters={'colsample_bytree': [0.6, 0.8],\n",
      "                                            'gamma': [2], 'max_depth': [3],\n",
      "                                            'min_child_weight': [3],\n",
      "                                            'random_state': [1005],\n",
      "                                            'subsample': [0.6, 0.8]}))])\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069d2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('processing', <transformers.RawDataProcessor object at 0x715689754050>), ('slice_columns', <transformers.DataSlicer object at 0x7156897570e0>), ('null_filling', <transformers.NullFillTransformer object at 0x715689757e00>), ('model', FitModel(folds=5,\n",
      "         hyper_parameters={'colsample_bytree': [0.6, 0.8], 'gamma': [2],\n",
      "                           'max_depth': [3], 'min_child_weight': [3],\n",
      "                           'random_state': [1005], 'subsample': [0.6, 0.8]}))], 'verbose': False, 'processing': <transformers.RawDataProcessor object at 0x715689754050>, 'slice_columns': <transformers.DataSlicer object at 0x7156897570e0>, 'null_filling': <transformers.NullFillTransformer object at 0x715689757e00>, 'model': FitModel(folds=5,\n",
      "         hyper_parameters={'colsample_bytree': [0.6, 0.8], 'gamma': [2],\n",
      "                           'max_depth': [3], 'min_child_weight': [3],\n",
      "                           'random_state': [1005], 'subsample': [0.6, 0.8]}), 'model__folds': 5, 'model__hyper_parameters': {'max_depth': [3], 'min_child_weight': [3], 'subsample': [0.6, 0.8], 'colsample_bytree': [0.6, 0.8], 'gamma': [2], 'random_state': [1005]}}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e09fa5",
   "metadata": {},
   "source": [
    "## Prediction Test with Sample Data\n",
    "This cell creates sample input data in the format our model expects and makes a test prediction:\n",
    "- We create a DataFrame with all required features\n",
    "- Use placeholder values that match the expected data types\n",
    "- The prediction output helps verify the model works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a127c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    pipeline = pickle.load(f)\n",
    "\n",
    "# Assuming 'decline_v2a_debit' is one of the required features\n",
    "input_data = pd.DataFrame({\n",
    "    'timestamp': ['2023-05-01'],\n",
    "    'in_data': ['{\"yams_score\":0.7,\"north_star_metric\":\"5.5\"}'],\n",
    "    'decline_v2a_debit': [0.5],\n",
    "    'days_since_sms_otp_success': [20],\n",
    "    'days_since_receiver_first_seen': [100],\n",
    "    'days_since_device_first_seen': [20],\n",
    "    'dda_age_in_days': [100]# Add this and any other missing features\n",
    "    # ... add all other required features ...\n",
    "})\n",
    "\n",
    "# Make a prediction\n",
    "prediction = pipeline.predict(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e38fd",
   "metadata": {},
   "source": [
    "## Prediction Result Verification\n",
    "Simply prints the output of our test prediction. This helps confirm that:\n",
    "- The model is working\n",
    "- The output format is as expected\n",
    "- There are no immediate errors in the prediction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373d7468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uncalibrated': array([[0.14639568, 0.8536043 ]], dtype=float32), 'calibrated': array([[0.60676062, 0.39323938]])}\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7faf44",
   "metadata": {},
   "source": [
    "## API Endpoint Simulation\n",
    "This cell simulates how our model would handle requests when deployed as an API endpoint:\n",
    "- Creates a mock HTTP request\n",
    "- Processes it through the model's invoke method\n",
    "- Shows how input data would be received and processed in a production environment\n",
    "This helps test our serving code before actual deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac7e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Contents of /opt/ml:\n",
      "└── ml/\n",
      "    └── model/\n",
      "        ├── test.txt\n",
      "        └── model.pkl\n",
      "Current working directory: /home/murivirg/work/github/sagemaker-tutorials/inference_expert_solution_with_transformers\n",
      "└── inference_expert_solution_with_transformers/\n",
      "    ├── predict.py\n",
      "    ├── transformers.py\n",
      "    ├── requirements.txt\n",
      "    ├── main.ipynb\n",
      "    ├── model.tar.gz\n",
      "    ├── __pycache__/\n",
      "    │   ├── transformers.cpython-313.pyc\n",
      "    │   └── predict.cpython-313.pyc\n",
      "    ├── main.py\n",
      "    ├── env/ (Python virtual environment, contents not listed)\n",
      "    ├── model.pkl\n",
      "    ├── model/\n",
      "    │   ├── model.pkl\n",
      "    │   └── .ipynb_checkpoints/\n",
      "    ├── documented_main.ipynb\n",
      "    ├── docker-pulled-image-as-base/\n",
      "    │   ├── ecr_test.sh\n",
      "    │   ├── dockerfile\n",
      "    │   └── .ipynb_checkpoints/\n",
      "    │       ├── dockerfile-checkpoint\n",
      "    │       └── ecr_test-checkpoint.sh\n",
      "    └── .ipynb_checkpoints/\n",
      "        ├── documented_main-checkpoint.ipynb\n",
      "        ├── requirements-checkpoint.txt\n",
      "        ├── predict-checkpoint.py\n",
      "        ├── transformers-checkpoint.py\n",
      "        ├── main-checkpoint.ipynb\n",
      "        └── main-checkpoint.py\n",
      "Script directory: /home/murivirg/work/github/sagemaker-tutorials/inference_expert_solution_with_transformers\n",
      "└── inference_expert_solution_with_transformers/\n",
      "    ├── predict.py\n",
      "    ├── transformers.py\n",
      "    ├── requirements.txt\n",
      "    ├── main.ipynb\n",
      "    ├── model.tar.gz\n",
      "    ├── __pycache__/\n",
      "    │   ├── transformers.cpython-313.pyc\n",
      "    │   └── predict.cpython-313.pyc\n",
      "    ├── main.py\n",
      "    ├── env/ (Python virtual environment, contents not listed)\n",
      "    ├── model.pkl\n",
      "    ├── model/\n",
      "    │   ├── model.pkl\n",
      "    │   └── .ipynb_checkpoints/\n",
      "    ├── documented_main.ipynb\n",
      "    ├── docker-pulled-image-as-base/\n",
      "    │   ├── ecr_test.sh\n",
      "    │   ├── dockerfile\n",
      "    │   └── .ipynb_checkpoints/\n",
      "    │       ├── dockerfile-checkpoint\n",
      "    │       └── ecr_test-checkpoint.sh\n",
      "    └── .ipynb_checkpoints/\n",
      "        ├── documented_main-checkpoint.ipynb\n",
      "        ├── requirements-checkpoint.txt\n",
      "        ├── predict-checkpoint.py\n",
      "        ├── transformers-checkpoint.py\n",
      "        ├── main-checkpoint.ipynb\n",
      "        └── main-checkpoint.py\n",
      "Received request body: {\"timestamp\": \"2023-05-01\", \"in_data\": \"{\\\"yams_score\\\":0.7,\\\"north_star_metric\\\":\\\"5.5\\\"}\", \"decline_v2a_debit\": 0.5, \"days_since_sms_otp_success\": 20, \"days_since_receiver_first_seen\": 100, \"days_since_device_first_seen\": 20, \"dda_age_in_days\": 100}\n",
      "Parsed input: {'timestamp': '2023-05-01', 'in_data': '{\"yams_score\":0.7,\"north_star_metric\":\"5.5\"}', 'decline_v2a_debit': 0.5, 'days_since_sms_otp_success': 20, 'days_since_receiver_first_seen': 100, 'days_since_device_first_seen': 20, 'dda_age_in_days': 100}\n",
      "Input DataFrame:     timestamp                                       in_data  \\\n",
      "0  2023-05-01  {\"yams_score\":0.7,\"north_star_metric\":\"5.5\"}   \n",
      "\n",
      "   decline_v2a_debit  days_since_sms_otp_success  \\\n",
      "0                0.5                          20   \n",
      "\n",
      "   days_since_receiver_first_seen  days_since_device_first_seen  \\\n",
      "0                             100                            20   \n",
      "\n",
      "   dda_age_in_days  \n",
      "0              100  \n",
      "Prediction: {'uncalibrated': array([[0.14639568, 0.8536043 ]], dtype=float32), 'calibrated': array([[0.60676062, 0.39323938]])}\n",
      "Type of prediction: <class 'dict'>\n",
      "Keys in prediction: ['uncalibrated', 'calibrated']\n",
      "Calibrated prediction shape: (1, 2)\n",
      "Response: {'prediction': [0.6067606151103974, 0.39323938488960264]}\n",
      "Response: {'prediction': [0.6067606151103974, 0.39323938488960264]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from predict import MyModel\n",
    "\n",
    "# Load the model from the current directory\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model_pipeline = pickle.load(f)\n",
    "\n",
    "# Create an instance of MyModel without calling __init__\n",
    "model_instance = MyModel.__new__(MyModel)\n",
    "model_instance.model = model_pipeline\n",
    "\n",
    "# Define a mock request class to simulate HTTPServerRequest\n",
    "class MockRequest:\n",
    "    def __init__(self, body):\n",
    "        self.body = body\n",
    "\n",
    "# Prepare input data as a dictionary (adjust as per your model's requirements)\n",
    "input_data = {\n",
    "    'timestamp': '2023-05-01',\n",
    "    'in_data': '{\"yams_score\":0.7,\"north_star_metric\":\"5.5\"}',\n",
    "    'decline_v2a_debit': 0.5,\n",
    "    'days_since_sms_otp_success': 20,\n",
    "    'days_since_receiver_first_seen': 100,\n",
    "    'days_since_device_first_seen': 20,\n",
    "    'dda_age_in_days': 100\n",
    "}\n",
    "\n",
    "# Convert input data to JSON string and encode to bytes\n",
    "json_input = json.dumps(input_data)\n",
    "mock_request = MockRequest(json_input.encode('utf-8'))\n",
    "\n",
    "# Call the invoke method and get the response\n",
    "response_bytes = model_instance.invoke(mock_request)\n",
    "\n",
    "# Decode and parse the response\n",
    "response_str = response_bytes.decode('utf-8')\n",
    "response_json = json.loads(response_str)\n",
    "\n",
    "# Print the result\n",
    "print(\"Response:\", response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c573ba9",
   "metadata": {},
   "source": [
    "## Model Deployment Preparation\n",
    "Here we upload all necessary files to Amazon S3:\n",
    "- The trained model (model.pkl)\n",
    "- Python dependencies (requirements.txt)\n",
    "- Custom prediction code (predict.py, transformers.py)\n",
    "SageMaker will use predict.py files to create a deployable package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88377b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "prefix = 'test/sagemaker/inference-expert-solution-with-transformers'\n",
    "# Upload the tar.gz file to S3\n",
    "s3.upload_file(\"model.pkl\", bucket, f\"{prefix}/model.pkl\")\n",
    "s3.upload_file(\"requirements.txt\", bucket, f\"{prefix}/requirements.txt\")\n",
    "s3.upload_file(\"predict.py\", bucket, f\"{prefix}/predict.py\")\n",
    "s3.upload_file(\"transformers.py\", bucket, f\"{prefix}/transformers.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853b914",
   "metadata": {},
   "source": [
    "## Upload Verification\n",
    "This cell confirms that our files were successfully uploaded to S3. It lists all files in the specified S3 path to ensure our deployment package is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9362a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/sagemaker/inference-expert-solution-with-transformers/\n",
      "test/sagemaker/inference-expert-solution-with-transformers/model.pkl\n",
      "test/sagemaker/inference-expert-solution-with-transformers/predict.py\n",
      "test/sagemaker/inference-expert-solution-with-transformers/requirements.txt\n",
      "test/sagemaker/inference-expert-solution-with-transformers/transformers.py\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=prefix\n",
    ")\n",
    "\n",
    "# Print all objects in the folder\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749b158-cc68-420f-af05-c3a2635084b2",
   "metadata": {},
   "source": [
    "## creation of the ECR repository\n",
    "\n",
    "This process can be easily done. But I provided a script to facilitate this process even more.\n",
    "\n",
    "### steps\n",
    "1. cd into docker-pulled-image-as-base directory\n",
    "```\n",
    "cd docker-pulled-image-as-base\n",
    "```\n",
    "2. update the necessary variables in ecr_test.sh\n",
    "```\n",
    "AWS_ACCOUNT_ID=\"794038231401\"  # Replace with your AWS account ID\n",
    "REGION=\"us-east-1\"              # Replace with your region\n",
    "```\n",
    "3. build the docker image\n",
    "```\n",
    "sh ecr_test.sh\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ddac85",
   "metadata": {},
   "source": [
    "## SageMaker Model Creation\n",
    "Here we define the SageMaker Model object:\n",
    "- Specifies the Docker container image from ECR\n",
    "- Points to our model files in S3\n",
    "- Sets up environment variables\n",
    "- Uses the appropriate IAM role\n",
    "This is the blueprint SageMaker will use to deploy our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac666dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import pandas as pd\n",
    "import json\n",
    "### IMPORTANT you need to update with your own variables\n",
    "# Get the SageMaker execution role (assumes this is run in a SageMaker notebook)\n",
    "role = \"arn:aws:iam::794038231401:role/service-role/SageMaker-ExecutionRole-20250103T203496\"\n",
    "\n",
    "# Specify your ECR image URI (replace with your actual URI)\n",
    "ecr_image = '794038231401.dkr.ecr.us-east-1.amazonaws.com/custom-base-model-20250502135641:latest'\n",
    "model_name = \"Custom-model-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "model_data = 's3://mainbucketrockhight5461/test/sagemaker/inference-expert-solution-with-transformers/'\n",
    "\n",
    "# Create the SageMaker model\n",
    "env_vars = {'SAGEMAKER_INFERENCE_CODE':'predict.handler'}\n",
    "\n",
    "model = Model(\n",
    "    name =  model_name,\n",
    "    image_uri = ecr_image,\n",
    "    env = env_vars,\n",
    "    model_data={\n",
    "       \"S3DataSource\": {\n",
    "          \"S3Uri\": model_data,\n",
    "          \"S3DataType\": \"S3Prefix\",\n",
    "          \"CompressionType\": \"None\"\n",
    "       }\n",
    "    },\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db38dd",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "This cell actually deploys our model to a SageMaker endpoint:\n",
    "- Creates compute resources (ML instance)\n",
    "- Loads our container and model\n",
    "- Makes the model available via a REST API endpoint\n",
    "Deployment typically takes 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "656f7dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/02/25 14:55:22] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating model with name: Custom-model-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">05</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>             <a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#4094\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4094</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/02/25 14:55:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating model with name: Custom-model-\u001b[1;36m2025\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m20\u001b[0m-\u001b[1;36m55\u001b[0m-\u001b[1;36m20\u001b[0m             \u001b]8;id=570066;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=850886;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#4094\u001b\\\u001b[2m4094\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/02/25 14:55:23] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint-config with name Custom-endpoint-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">05</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span> <a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#5937\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5937</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/02/25 14:55:23]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint-config with name Custom-endpoint-\u001b[1;36m2025\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m20\u001b[0m-\u001b[1;36m55\u001b[0m-\u001b[1;36m22\u001b[0m \u001b]8;id=850930;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=345959;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#5937\u001b\\\u001b[2m5937\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/02/25 14:55:24] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating endpoint with name Custom-endpoint-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">05</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span>        <a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#4759\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4759</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/02/25 14:55:24]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating endpoint with name Custom-endpoint-\u001b[1;36m2025\u001b[0m-\u001b[1;36m05\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m20\u001b[0m-\u001b[1;36m55\u001b[0m-\u001b[1;36m22\u001b[0m        \u001b]8;id=695831;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=48328;file:///home/murivirg/.pyenv/versions/3.13.1/lib/python3.13/site-packages/sagemaker/session.py#4759\u001b\\\u001b[2m4759\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to an endpoint\n",
    "\n",
    "endpoint_name = \"Custom-endpoint-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',  # Adjust instance type as needed\n",
    "    endpoint_name=endpoint_name   # Replace with a unique endpoint name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22589404",
   "metadata": {},
   "source": [
    "## Endpoint Testing\n",
    "Finally, we test our deployed endpoint:\n",
    "- Send sample data in the correct JSON format\n",
    "- Verify we get back expected predictions\n",
    "- Test both single and batch predictions\n",
    "This confirms our entire deployment pipeline works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90db4154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single input response: {'prediction': [0.6067606151103974, 0.39323938488960264]}\n",
      "Multiple inputs response: {'predictions': [[0.6067606151103974, 0.39323938488960264], [0.6067606151103974, 0.39323938488960264]]}\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Create the predictor with JSON serializer and deserializer\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "# Prepare input data as a dictionary\n",
    "input_data = {\n",
    "    'timestamp': '2023-05-01',\n",
    "    'in_data': '{\"yams_score\":0.7,\"north_star_metric\":\"5.5\"}',\n",
    "    'decline_v2a_debit': 0.5,\n",
    "    'days_since_sms_otp_success': 20,\n",
    "    'days_since_receiver_first_seen': 100,\n",
    "    'days_since_device_first_seen': 20,\n",
    "    'dda_age_in_days': 100\n",
    "}\n",
    "\n",
    "# Test with single input\n",
    "response = predictor.predict(input_data)\n",
    "print(\"Single input response:\", response)\n",
    "\n",
    "# Test with multiple inputs (list of dictionaries)\n",
    "input_data_list = [input_data, input_data]\n",
    "response = predictor.predict(input_data_list)\n",
    "print(\"Multiple inputs response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d22de-1653-451e-8ff6-823586e6124f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
